{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load CSV data to BigQuery Staging Tables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> This script run daily to read csv file in data/<br>\n",
    "> On each completion, csv files will be moved to data_backup/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-29T00:10:06.063786Z",
     "iopub.status.busy": "2025-03-29T00:10:06.063197Z",
     "iopub.status.idle": "2025-03-29T00:11:02.964778Z",
     "shell.execute_reply": "2025-03-29T00:11:02.963104Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import sys\n",
    "import shutil\n",
    "import logging\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from google.cloud import bigquery\n",
    "from google.cloud.exceptions import NotFound\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# Paths and Settings\n",
    "# ------------------------------------------------------------------------------\n",
    "csv_folder = \"../data\"\n",
    "backup_folder = \"../data_backup\"\n",
    "log_folder = \"../logs\"\n",
    "project_id = \"olist-ecommerce-454812\"  # or \"horace-integration-453108\"\n",
    "staging_dataset = \"olist_data_staging\"\n",
    "\n",
    "# Create backup folder, if it exists, skipped\n",
    "if not os.path.exists(backup_folder):\n",
    "    os.makedirs(backup_folder)\n",
    "\n",
    "# Create logs folder, if it exists, skipped\n",
    "if not os.path.exists(log_folder):\n",
    "    os.makedirs(log_folder)\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# Configure Logging\n",
    "# ------------------------------------------------------------------------------\n",
    "log_date = datetime.today().strftime('%Y_%m_%d')\n",
    "log_filename = os.path.join(log_folder, f\"csv_to_staging_{log_date}.log\")\n",
    "\n",
    "logging.basicConfig(\n",
    "    filename=log_filename,\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s [%(levelname)s] %(message)s\",\n",
    "    datefmt=\"%Y-%m-%d %H:%M:%S\"\n",
    ")\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# BigQuery Client + Connectivity Check\n",
    "# ------------------------------------------------------------------------------\n",
    "try:\n",
    "    bq_client = bigquery.Client(project=project_id)\n",
    "    # Attempt to list datasets as a connectivity check\n",
    "    _ = list(bq_client.list_datasets())\n",
    "    logging.info(\"Connected to BigQuery successfully.\")\n",
    "except Exception as e:\n",
    "    logging.error(f\"Failed to connect to BigQuery: {e}\")\n",
    "    sys.exit(1)\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# Regex to match CSV files like: <filename>_YYYY_MM_DD.csv\n",
    "# ------------------------------------------------------------------------------\n",
    "pattern = re.compile(r\"^(.*)_\\d{4}_\\d{2}_\\d{2}\\.csv$\")\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# Process Each CSV File\n",
    "# ------------------------------------------------------------------------------\n",
    "for file in os.listdir(csv_folder):\n",
    "    file_path = os.path.join(csv_folder, file)\n",
    "    match = pattern.match(file)\n",
    "    if match:\n",
    "        base_table_name = match.group(1)\n",
    "        logging.info(f\"Processing file: {file} -> Target table: {base_table_name}\")\n",
    "        \n",
    "        # Read CSV\n",
    "        try:\n",
    "            df = pd.read_csv(file_path)\n",
    "            logging.info(f\"Read CSV with shape: {df.shape}\")\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error reading {file}: {e}\")\n",
    "            continue\n",
    "        \n",
    "        # Skip empty DataFrame\n",
    "        if df.empty:\n",
    "            logging.info(f\"File {file} is empty. Skipping ingestion and moving to backup.\")\n",
    "            try:\n",
    "                shutil.move(file_path, os.path.join(backup_folder, file))\n",
    "                logging.info(f\"Moved empty file {file} to backup folder.\")\n",
    "            except Exception as move_err:\n",
    "                logging.error(f\"Error moving empty file {file} to backup folder: {move_err}\")\n",
    "            continue\n",
    "        \n",
    "        # ------------------------------------------------------------------------------\n",
    "        # Add a timestamp column (Python datetime) for BigQuery\n",
    "        # ------------------------------------------------------------------------------\n",
    "        df['last_updated'] = datetime.now()  # Python datetime object\n",
    "        \n",
    "        # Define BigQuery table ID\n",
    "        table_id = f\"{project_id}.{staging_dataset}.{base_table_name}\"\n",
    "        \n",
    "        # Check if table exists\n",
    "        try:\n",
    "            bq_client.get_table(table_id)\n",
    "            write_disposition = bigquery.WriteDisposition.WRITE_APPEND\n",
    "            logging.info(f\"BigQuery table {table_id} exists; appending data.\")\n",
    "        except NotFound:\n",
    "            write_disposition = bigquery.WriteDisposition.WRITE_TRUNCATE\n",
    "            logging.info(f\"BigQuery table {table_id} does not exist; it will be created.\")\n",
    "        \n",
    "        # Configure load job\n",
    "        job_config = bigquery.LoadJobConfig(\n",
    "            create_disposition=bigquery.CreateDisposition.CREATE_IF_NEEDED,\n",
    "            write_disposition=write_disposition,\n",
    "            autodetect=True,\n",
    "        )\n",
    "        \n",
    "        # Load data into BigQuery\n",
    "        try:\n",
    "            load_job = bq_client.load_table_from_dataframe(df, table_id, job_config=job_config)\n",
    "            load_job.result()  # Wait for the job to complete\n",
    "            logging.info(f\"Data successfully loaded into {table_id}.\")\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error loading data into BigQuery table {table_id}: {e}\")\n",
    "            continue\n",
    "        \n",
    "        # Move processed file to backup folder\n",
    "        try:\n",
    "            shutil.move(file_path, os.path.join(backup_folder, file))\n",
    "            logging.info(f\"Moved {file} to backup folder.\")\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error moving {file} to backup folder: {e}\")\n",
    "\n",
    "logging.info(\"CSV import process completed.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "elt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
